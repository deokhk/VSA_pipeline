{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aeac7fd9-f99f-4e01-9d92-15b4fd63d50f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2, socket, numpy, pickle, os \n",
    "s=socket.socket(socket.AF_INET,socket.SOCK_DGRAM)\n",
    "s.setsockopt(socket.SOL_SOCKET, socket.SO_SNDBUF, 10000000)\n",
    "serverip=\"0.0.0.1\"\n",
    "serverport=111"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e67b75ef-8ff3-47ed-955c-7a4da2f7f8a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import cv2\n",
    "import argparse\n",
    "import time\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "473e46b6-9a18-40ee-b524-261b047eab82",
   "metadata": {},
   "outputs": [],
   "source": [
    "names = \"/nvdli-nano/data/action_recognition_kinetics.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8dcc52f5-608a-465c-adbd-f159499a4181",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of frames to consider for each prediction: 16\n"
     ]
    }
   ],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "\n",
    "parser.add_argument('-f', '--file', required=False)\n",
    "parser.add_argument('-l', '--clip-len', dest='clip_len', default=16, type=int,\n",
    "                    help='number of frames to consider for each prediction')\n",
    "parser.add_argument('-c', '--classes', default = names, help='Path to classes list.')\n",
    "\n",
    "args = vars(parser.parse_args())\n",
    "#### PRINT INFO #####\n",
    "print(f\"Number of frames to consider for each prediction: {args['clip_len']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "82dcc466-d1d8-4dde-92e7-708928290df9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device is cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/r2plus1d_18-91a641e6.pth\" to /root/.cache/torch/hub/checkpoints/r2plus1d_18-91a641e6.pth\n",
      "100.0%\n"
     ]
    }
   ],
   "source": [
    "# get the lables\n",
    "class_names = open(args[\"classes\"]).read().strip().split(\"\\n\")\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Device is {device}\")\n",
    "\n",
    "# load the model\n",
    "model = torchvision.models.video.r2plus1d_18(pretrained=True, progress=True) #This one works.\n",
    "\n",
    "# load the model onto the computation device\n",
    "model = model.eval().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "791c2a88-9489-49de-9bfa-2707b240c477",
   "metadata": {},
   "outputs": [],
   "source": [
    "from jetcam.csi_camera import CSICamera\n",
    "fps_origin = 30\n",
    "#camera = CSICamera(width=224, height=224)\n",
    "\n",
    "camera = CSICamera(width=400, height=225, capture_width=1280, capture_height=720, capture_fps=fps_origin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7b9fd50d-690d-46cf-9b1d-a6547dfc95c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "frame_count = 0 # to count total frames\n",
    "total_fps = 0 # to get the final frames per second\n",
    "# a clips list to append and store the individual frames\n",
    "clips = []\n",
    "frame_and_metadatas = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4512aaf8-cd9b-4f33-8b19-2f118e75541f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms as T\n",
    "# define the transforms\n",
    "# This cell may need to be ran twice, ignore the first run error.\n",
    "transform = T.Compose([\n",
    "    T.Resize((128, 171)),\n",
    "    T.CenterCrop((112, 112)),\n",
    "    T.Normalize(mean = [0.43216, 0.394666, 0.37645],\n",
    "                std = [0.22803, 0.22145, 0.216989], )\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11739c7c-6548-410b-a7aa-5effb070b780",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read until end of video\n",
    "from google.colab.patches import cv2_imshow\n",
    "processed=0\n",
    "while(cap.isOpened()):\n",
    "    # capture each frame of the video\n",
    "    frame = camera.read()\n",
    "    # get the start time\n",
    "    start_time = time.time()\n",
    "    image = frame.copy()\n",
    "    frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    frame = transform(image=frame)['image']\n",
    "    clips.append(frame)\n",
    "    if len(clips) == args['clip_len']:\n",
    "        with torch.no_grad(): # we do not want to backprop any gradients\n",
    "            input_frames = np.array(clips)\n",
    "            # add an extra dimension        \n",
    "            input_frames = np.expand_dims(input_frames, axis=0)\n",
    "            # transpose to get [1, 3, num_clips, height, width]\n",
    "            input_frames = np.transpose(input_frames, (0, 4, 1, 2, 3))\n",
    "            # convert the frames to tensor\n",
    "            input_frames = torch.tensor(input_frames, dtype=torch.float32)\n",
    "            input_frames = input_frames.to(device)\n",
    "            # forward pass to get the predictions\n",
    "            outputs = model(input_frames)\n",
    "            # get the prediction index\n",
    "            _, preds = torch.max(outputs.data, 1)\n",
    "\n",
    "            # map predictions to the respective class names\n",
    "            label = class_names[preds].strip()\n",
    "        # get the end time\n",
    "        end_time = time.time()\n",
    "        # get the fps\n",
    "        fps = 1 / (end_time - start_time)\n",
    "        # add fps to total fps\n",
    "        total_fps += fps\n",
    "        # increment frame count\n",
    "        frame_count += 1\n",
    "        wait_time = max(1, int(fps/4))\n",
    "        label_with_time_stamp = label + f\"  frame #: {frame_count}, time: {(cap.get(cv2.CAP_PROP_POS_MSEC))/1000:.2f}s\"\n",
    "        cv2.putText(image, label_with_time_stamp, (15, 60),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 1.5, (0, 0, 255), 2, \n",
    "                    lineType=cv2.LINE_AA)\n",
    "        clips.pop(0)\n",
    "        if frame_count % 20 == 0:\n",
    "        #     cv2_imshow(image) # colab이 아닌 그냥 Ipynb에서는 cv.imshow() 쓰면 돼요\n",
    "          ret, buffer = cv2.imencode(\".jpg\", image, [int(cv2.IMWRITE_JPEG_QUALITY), 30])    \n",
    "          x_as_bytes = pickle.dumps(buffer)    \n",
    "\n",
    "          s.sendto(x_as_bytes,(serverip , serverport))\n",
    "        # out.write(image)\n",
    "\n",
    "        # if label.strip() not in category2group:\n",
    "        #     group = \"others\"\n",
    "        # else:\n",
    "        #     group = category2group[label.strip()]\n",
    "        # meta_dict = {\"label\":label, \"pos\":frame_count, \"group\":group}\n",
    "        # frame_and_metadatas.append(meta_dict)\n",
    "\n",
    "        # press `q` to exit\n",
    "        if cv2.waitKey(wait_time) & 0xFF == ord('q'):\n",
    "            break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
